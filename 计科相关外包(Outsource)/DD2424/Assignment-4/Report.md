## Report

#### i) State how you checked your analytic gradient computations and whether you think that your gradient computations are bug free for your RNN.

+ è§£:

  é¦–å…ˆæˆ‘ä»¬æ„å»ºRNNçš„æ•°å­¦è¡¨è¾¾å½¢å¼, å…¶ä¸­è¾“å…¥ $x \in \mathbb{R}^d$, éšå«å±‚ç»´åº¦ä¸º $m$, è¾“å‡ºå±‚ç»“ç‚¹æ•°(ç±»åˆ«æ•°)ä¸º $K$, æˆ‘ä»¬æœ‰ å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸º:
  $$
  \begin{aligned}
  \mathbf{a}_{t} &=W \mathbf{h}_{t-1}+U \mathbf{x}_{t}+\mathbf{b} \\
  \mathbf{h}_{t} &=\tanh \left(\mathbf{a}_{t}\right) \\
  \mathbf{o}_{t} &=V \mathbf{h}_{t}+\mathbf{c} \\
  \mathbf{p}_{t} &=\operatorname{SoftMax}\left(\mathbf{o}_{t}\right)
  \end{aligned}
  $$
  å…¶ä¸­ $\mathbf{x}_t$ ä¸ºç¬¬ $t$ ä¸ªæ—¶é—´æ­¥çš„è¾“å…¥. æŒ‰ç…§ä¸Šé¢çš„å‰å‘ä¼ æ’­æ•°å­¦è¡¨è¾¾, ä»£ç å®ç°ä¸º:

  ```python
  for t in range(len(X_in)):
      x_ti = np.expand_dims(X_hot[:, t], axis=1)
      x[t] = np.copy(x_ti)
      a = np.dot(W, h_store[t - 1]) + np.dot(U, x[t]) + b
      h_store[t] = np.tanh(a)
      out[t] = np.dot(V, h_store[t]) + c
      softmax[t] = np.exp(out[t]) / np.sum(np.exp(out[t]))
  
      y_ti = np.expand_dims(Y_hot[:, t], axis=1)
      y[t] = np.copy(y_ti)
      loss += -np.log(np.dot(y[t].T, softmax[t]))[0][0]
  ```

  è¯·æ³¨æ„RNNä¸­æ¯ä¸€æ¬¡çš„è¾“å…¥ä¸ä¸Šä¸€å±‚æ—¶é—´æ­¥çš„è¾“å‡ºæœ‰å…³, è¿™ä½“ç°åœ¨éšå«å±‚çš„è¾“å…¥ä¸­:

  ```python
  a = np.dot(W, h_store[t - 1]) + np.dot(U, x[t]) + b
  h_store[t] = np.tanh(a)
  ```

  æœ€åæŸå¤±å‡½æ•°æ¥åˆ°äº¤å‰ç†µ:
  $$
  L\left(\mathrm{x}_{1: \tau}, \mathrm{y}_{1: \tau}, \Theta\right)=\sum_{t=1}^{\tau} l_{t}=-\sum_{t=1}^{\tau} \log \left(\mathrm{y}_{t}^{T} \mathrm{p}_{t}\right)
  $$
  åœ¨PPTå†…æœ‰ç›´è§‚è¯¦ç»†çš„ä¾‹å­: ![image-20201001173347353](assets/image-20201001173347353.png)

  

  + åå‘ä¼ æ’­è¿‡ç¨‹ Back-prop for a vanilla RNN (è¯¦ç»†è¯·è§ PPT `Lecture9.pdf`):
    $$
    \frac{\partial L}{\partial V}=\sum_{t=1}^{\tau} \mathbf{g}_{t}^{T} \mathbf{h}_{t}^{T} \\
    \frac{\partial L}{\partial \mathbf{h}_{\tau}}=\frac{\partial L}{\partial \mathbf{o}_{\tau}} V \\
    \frac{\partial L}{\partial \mathbf{h}_{t}}=\frac{\partial L}{\partial \mathbf{o}_{t}} V+\frac{\partial L}{\partial \mathbf{a}_{t+1}} W \\
    \frac{\partial L}{\partial \mathbf{a}_{t}}=\frac{\partial L}{\partial \mathbf{h}_{t}} \operatorname{diag}\left(1-\tanh ^{2}\left(\mathbf{a}_{t}\right)\right) \\
    \frac{\partial L}{\partial W}=\sum_{t=1}^{\tau} \mathbf{g}_{t}^{T} \mathbf{h}_{t-1}^{T} \\
    \frac{\partial L}{\partial U}=\sum_{t=1}^{\tau} \mathbf{g}_{t}^{T} \mathbf{x}_{t}^{T}
    $$
    ç”±ä¸Šè¿°ç»“æœ, åœ¨æ¯ä¸ªè¿­ä»£å†…è®¡ç®—æ¢¯åº¦çš„ä»£ç å®ç°ä¸º:

    ```python
    for t in reversed(range(len(X_in))):
        grad_o = softmax[t]
        grad_o[Y_out[t]] -= 1
    
        # hidden -> output (weight and bias)
        grad_V += np.dot(grad_o, h_store[t].T)
        grad_c += grad_o
    
        grad_h = np.dot(V.T, grad_o) + np.dot(W.T, grad_a)
        grad_a = grad_h * (1 - (h_store[t] ** 2))
    
        # hidden -> hidden (weight and bias)
        grad_W += np.dot(grad_a, h_store[t - 1].T)
        grad_b += grad_a
    
        # input -> hidden (weight)
        grad_U += np.dot(grad_a, x[t].T)
    ```

    ä»£ç å®ç°ä¸æ•°å­¦æ¨å¯¼çš„ç»“æœæ˜¯ä¸€ä¸€å¯¹åº”çš„, è¿™å¯ä»¥è¯æ˜æ¢¯åº¦çš„æ±‚è§£æ˜¯æ­£ç¡®çš„.

    

  + åŒæ—¶æˆ‘è¿˜ä½¿ç”¨äº†elementçº§åˆ«çš„æ±‚è§£æ¢¯åº¦, å¹¶å°†ç»“æœä¸ä¸Šè¿°æ¯”è¾ƒ, ä»¥è¯æ˜æ¢¯åº¦æ±‚è§£çš„æ­£ç¡®æ€§.

    ä¾‹å¦‚å¯¹äº $U$ çš„è®¡ç®—ä¸º:

    ```python
    for i in range(U.shape[0]):
        for j in range(U.shape[1]):
            U_try = np.copy(U)
            U_try[i, j] -= h
            weights_try = (b, c, U_try, W, V)
            c1 = self.RNN.compute_loss(X_in, Y_out, X_hot, Y_hot, hprev, U, W, V, b, c)[5]
            U_try = np.copy(U)
            U_try[i, j] += h
            weights_try = (b, c, U_try, W, V)
            c2 = self.RNN.compute_loss(X_in, Y_out, X_hot, Y_hot, hprev, U, W, V, b, c)[5]
            grad_U_num[i, j] = (c2 - c1) / (2 * h)
    ```

    å¯¹äº $V$ çš„è®¡ç®—ä¸º:

    ```python
    for i in range(V.shape[0]):
        for j in range(V.shape[1]):
            V_try = np.copy(V)
            V_try[i, j] -= h
            weights_try = (b, c, U, W, V_try)
            c1 = self.RNN.compute_loss(X_in, Y_out, X_hot, Y_hot, hprev, U, W, V, b, c)[5]
            V_try = np.copy(V)
            V_try[i, j] += h
            weights_try = (b, c, U, W, V_try)
            c2 = self.RNN.compute_loss(X_in, Y_out, X_hot, Y_hot, hprev, U, W, V, b, c)[5]
            grad_V_num[i, j] = (c2 - c1) / (2 * h);
    
    ```

    

#### ii) Include a graph of the smooth loss function for a longish training run (at least 2 epochs).

+ è§£:

  500 000 ä¸ªiteration:![](./figs/Goblet/2-1.png)

  è¯¦ç»†åˆ†æå‰100 000ä¸ªiteration: ![](./figs/Goblet/2-2.png)



#### iii) Show the evolution of the text synthesized by your RNN during training by including a sample of synthesized text (200 characters long) before the first and before every 10,000th update steps when you train for 100,000 update steps.

+ è§£:

##### Goblet:

  `1 iteration, loss = 89.10008475048072`:

  ```
  ,KwHuapaJ6,bxh)71ï»¿ITgDB6a'K0j7IZm/xpQ"M7P/xâ€¢Aj^iyas;PV4Q/j
gt.j/Jx)aH9pvA)s}_3yLF?HTox:q-WgaBPIv"pN	4Lo"oqW!}C}NJdX;?YZ)i6Z(MUdklCÃ¼AR;mJ_tZJ'4CX3ui3L0js7Gpt9CU)!x
  RbUXpQPdVzbHt0kD2ixKQ,JCt?Bâ€¢.m)Vh
ev
  ```

  `10 000 iteration, loss = 37.836761648863344`: 

  ```
lfcell, -ed then the wingodde groumed qumus the sperind to the giched who nimuser in sIotr warry car ppies.  "Yhe depprent stofn turmingereds sIop larg rravedtwumr tur at in the said the plious and si
  ```

  `20 000 iteration, loss = 33.20336693275177`:

  ```
or in the gues fade frullicge iternione, I harch rose the Krot hit'fled. The freghar to seid, wnaid thit her him herl.
  Fnt Hermioned a but suille. Crown the keadlogeore tane therplousougareed at miogh
  ```

  `100 000 iteration, loss = 30.941354829206706`:

```
  st's 'teving thes,'s don's gath in when.  Ayis somess, I all lest to sho right lon, but Chout!"
  . goingling you?" sowied ss itpenting the wine we darks. "AIt lou!"
  Harry ceveting mistine.
```

  

##### Trump:

  `1 iteration, loss = 100.90444758331577`:

  ```
F1ğŸ‡¸â¬‡WğŸ˜œğŸ‘¢â—â‚¬)MÃ©â€ğŸ˜±TÃ¡ğŸ˜‘ğŸ‘¿ğŸ«Â«ğŸ’ğŸ¢â˜‘ğŸ‡®bğŸ™ŒğŸ˜ğŸ˜´ğŸ™Œ$ğŸ‘âœ¨âœŒ.|ğŸ’°ğŸ¤–ğŸ‡±ğŸ‘—ğŸ˜ã€ŠğŸ‘”0NğŸ‘¢â˜K(ğŸ‘â‚¬ğŸ”…nÄºğŸš¨âŒÂ´XğŸ’Ã±ğŸ˜’â€ğŸ’¥â€•×OrğŸ»â€²ğŸ˜¢âŒgkâŒğŸ‘¿ğŸ½âœŒ
  ğŸ˜…ğŸ’¥ğŸ¼â˜4`ğŸ˜©ğŸ‘Œjqâ¤â†”
  ```

`10 000 iteration, loss = 44.29141982835848`: 

```
  ecoud fore the our ofttone in To0le nebed. fronpmirgredsl?-Ml He thent) http://t.dhet. Swamp is e.fpeat. junt do. In repealld hind ton orse
```

`20 000 iteration, loss = 40.70595130139549`:

  ```
  eekot!   Nadh muie non is verpira,, bee ruth Brear t.ay You nove I inn ane whire a toust for somes thath AmsonatÂ±@reAmbrial thump  Thofe  In
  ```

  `100 000 iteration, loss = 40.184938658121965`:

  ```
  Engronim: Odebampry rap sf in TrimT +od one love brew you for ay cryarm and you optred NY#makn7aldMnefthimG tome but wonf #Himprynow forc t
  ```



#### iv) A passage of length 1000 characters synthesized from your best model (the one that achieved the lowest loss).

+ è§£:

  $m = 100, \eta = .05$
  `smooth loss = 39.316939712621945`

```
Oh thent? stight you wourse the on Hagron. Itsore fanecn was hear was at prilly Rot pacce of cop belound gotter. Thissus," said Hall. It, he have fich the; I'm the tapt the horguble with sure at stos, Harry the viff, moobed thice ger thich herss mumend I pook dofe him araunden what's not uppont himdhanferudesweff to were too off was come kacang the fat be sour-bothil. Mang seathines... now reet the was be to moll? Ho him ugwe wake upple? The ither would to sever will amound his blaoned off bat masor My, Crol it had don-olle."
Harry as tref and ho cort had the at enty corting Horbent perell.. Harry, but muth sumby ulor tible his out vooned fro" was gourctor widem, waich fook seipped that evep tertata as Hagrid.
"What arout not to a ray it, geads. Chat him wood novely mether eorrut me watiss thit proorty. wood and fewerbown alaparmesild allic eare roffe upigel. "He had Grapice, and is, patred bobecat the wastly of the out and for tavarte, Harry folled. I'vary everiow o cow, bu
```



#### bonus

+ åŒºåˆ«:
  1. é¦–å…ˆæ•°æ®çš„è¯»å…¥ä»¥åŠé¢„å¤„ç†è‚¯å®šæ˜¯ä¸åŒçš„, ä¸€ä¸ªä»`txt`æ–‡ä»¶è¯»å…¥, ä¸€ä¸ªä»`json`æ–‡ä»¶è¯»å…¥, æŒ‰ç…§GitHubä¸Šç›¸å…³æŒ‡ç¤ºå³å¯.
  2. ç”Ÿæˆæ–‡æœ¬çš„é•¿åº¦ä¸ä¸€æ ·.
  3. åœ¨`Trump`ä¸­ç‰¹åˆ«åœ°å¯¹`Â±`è¿›è¡Œäº†å¤„ç†.